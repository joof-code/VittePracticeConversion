import os
import random
import json
import numpy as np

# Если вы будете использовать torch / tf позже — сразу заложим задел
try:
    import torch
except ImportError:
    torch = None

try:
    import tensorflow as tf
except ImportError:
    tf = None

# --- Подключение Google Drive (работает только в Colab) ---
try:
    from google.colab import drive  # type: ignore
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    drive.mount("/content/drive")
    BASE_DIR = "/content/drive/MyDrive/digital_marketing_conversion"
else:
    # Локальный fallback — можно поменять под себя
    BASE_DIR = os.path.abspath("./digital_marketing_conversion")

DATA_DIR = os.path.join(BASE_DIR, "data")
MODELS_DIR = os.path.join(BASE_DIR, "models")
PLOTS_DIR = os.path.join(BASE_DIR, "plots")
REPORTS_DIR = os.path.join(BASE_DIR, "reports")
ARTIFACTS_DIR = os.path.join(BASE_DIR, "artifacts")

for d in [BASE_DIR, DATA_DIR, MODELS_DIR, PLOTS_DIR, REPORTS_DIR, ARTIFACTS_DIR]:
    os.makedirs(d, exist_ok=True)

print("BASE_DIR:", BASE_DIR)
print("DATA_DIR:", DATA_DIR)
print("MODELS_DIR:", MODELS_DIR)
print("PLOTS_DIR:", PLOTS_DIR)
print("REPORTS_DIR:", REPORTS_DIR)
print("ARTIFACTS_DIR:", ARTIFACTS_DIR)

# --- Функция для установки случайных зерен ---
def set_global_seed(seed: int = 42) -> None:
    random.seed(seed)
    np.random.seed(seed)

    if torch is not None:
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        # Дополнительно чуть жёстче фиксируем детерминизм
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    if tf is not None:
        try:
            tf.random.set_seed(seed)
        except Exception:
            pass

    print(f"[INFO] Глобальное зерно случайности установлено: {seed}")

set_global_seed(42)

# --- Проверка наличия GPU ---
if torch is not None and torch.cuda.is_available():
    print("[INFO] PyTorch видит GPU:", torch.cuda.get_device_name(0))
elif tf is not None and len(tf.config.list_physical_devices("GPU")) > 0:
    print("[INFO] TensorFlow видит GPU:", tf.config.list_physical_devices("GPU"))
else:
    print("[WARN] GPU не обнаружен (или библиотеки DL ещё не установлены).")
    print("       Но для датасета на 8000 строк это не критично.")

# Базовые библиотеки анализа данных и визуализации
!pip install -q pandas numpy matplotlib seaborn

# ML-стек (дальше пригодится, но уже сейчас можно поставить)
!pip install -q scikit-learn imbalanced-learn xgboost lightgbm shap umap-learn

# Работа с Kaggle API и конфигами
!pip install -q kaggle pyyaml

import os
import zipfile
from pathlib import Path

if IN_COLAB:
    from google.colab import files  # type: ignore

    print(
        "Пожалуйста, загрузите kaggle.json (из профиля Kaggle → Account → Create New Token)."
    )
    uploaded = files.upload()  # Откроется диалог выбора файла

    if "kaggle.json" not in uploaded:
        raise FileNotFoundError(
            "Файл kaggle.json не найден среди загруженных. "
            "Убедитесь, что выбрали правильный файл."
        )

    # Настройка Kaggle API
    kaggle_dir = os.path.join(os.path.expanduser("~"), ".kaggle")
    os.makedirs(kaggle_dir, exist_ok=True)

    with open("kaggle.json", "r") as f_in:
        token = json.load(f_in)

    kaggle_json_path = os.path.join(kaggle_dir, "kaggle.json")
    with open(kaggle_json_path, "w") as f_out:
        json.dump(token, f_out)

    os.chmod(kaggle_json_path, 0o600)
    print("[INFO] kaggle.json настроен.")

    # Скачивание датасета
    print("[INFO] Скачиваю датасет 'Predict Conversion in Digital Marketing'...")
    !kaggle datasets download -d rabieelkharoua/predict-conversion-in-digital-marketing-dataset -p {DATA_DIR} --unzip

    # Проверим, какие CSV появились
    csv_files = list(Path(DATA_DIR).glob("*.csv"))
    print("[INFO] Найдены CSV-файлы в DATA_DIR:")
    for p in csv_files:
        print("   -", p.name)

else:
    print(
        "[WARN] Не в Colab: этот блок рассчитан на Google Colab и может не работать локально."
    )

import yaml

dataset_registry_path = os.path.join(DATA_DIR, "dataset_registry.yaml")

# Попробуем автоматически найти основной CSV
from glob import glob

csv_files = glob(os.path.join(DATA_DIR, "*.csv"))

main_dataset_path = None
for candidate in csv_files:
    name = os.path.basename(candidate).lower()
    if "digital_marketing_campaign" in name:
        main_dataset_path = candidate
        break

if main_dataset_path is None and csv_files:
    main_dataset_path = csv_files[0]  # fallback — первый найденный CSV

dataset_registry = {
    "main_conversion_dataset": {
        "source": "kaggle",
        "kaggle_dataset": "rabieelkharoua/predict-conversion-in-digital-marketing-dataset",
        "path": main_dataset_path,
        "description": "Predict Conversion in Digital Marketing Dataset (основной датасет для экспериментов)",
    },
    # Сюда позже можно дописать ещё один датасет (stuffmart, др.)
}

with open(dataset_registry_path, "w", encoding="utf-8") as f:
    yaml.safe_dump(dataset_registry, f, allow_unicode=True)

print("[INFO] dataset_registry.yaml сохранён по пути:", dataset_registry_path)
print("       Основной датасет:", main_dataset_path)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use("default")
sns.set_theme(style="whitegrid")

# Грузим registry, чтобы взять путь к датасету
with open(dataset_registry_path, "r", encoding="utf-8") as f:
    dataset_registry = yaml.safe_load(f)

main_dataset_info = dataset_registry["main_conversion_dataset"]
csv_path = main_dataset_info["path"]

if csv_path is None or not os.path.exists(csv_path):
    raise FileNotFoundError(
        f"CSV-файл основного датасета не найден. Ожидался путь: {csv_path}"
    )

print("[INFO] Загружаю основной датасет из:", csv_path)
df = pd.read_csv(csv_path)

print("\n[SHAPE] Размерность датасета:", df.shape)
print("\n[D_TYPES] Типы данных колонок:")
print(df.dtypes)

print("\n[HEAD] Первые 5 строк:")
display(df.head())

print("\n[NA] Количество пропусков в каждом столбце:")
print(df.isna().sum())

print("\n[DESCRIBE] Базовая статистика по числовым признакам:")
display(df.describe().T)

# --- Data Dictionary / Feature Catalog ---
feature_catalog = pd.DataFrame(
    {
        "feature_name": df.columns,
        "dtype": [str(dt) for dt in df.dtypes],
        "n_unique": [df[col].nunique() for col in df.columns],
        "n_missing": [df[col].isna().sum() for col in df.columns],
        # поле для ручного заполнения в отчёте при необходимости
        "description_ru": ["" for _ in df.columns],
        "example_value": [df[col].iloc[0] for col in df.columns],
    }
)

feature_catalog_path = os.path.join(DATA_DIR, "feature_catalog.csv")
feature_catalog.to_csv(feature_catalog_path, index=False, encoding="utf-8-sig")
print("\n[INFO] Каталог признаков сохранён в:", feature_catalog_path)
display(feature_catalog)

TARGET_COL = "Conversion"

if TARGET_COL not in df.columns:
    raise KeyError(
        f"Ожидалась колонка таргета '{TARGET_COL}', но её нет в df.columns: {df.columns.tolist()}"
    )

target_counts = df[TARGET_COL].value_counts().sort_index()
target_probs = df[TARGET_COL].value_counts(normalize=True).sort_index()

print("\n[INFO] Распределение целевой переменной (сырые значения):")
print(target_counts)
print("\n[INFO] Распределение целевой переменной (доли):")
print(target_probs)

fig, ax = plt.subplots(figsize=(5, 4))
sns.barplot(
    x=target_counts.index.astype(str),
    y=target_counts.values,
    ax=ax,
)
ax.set_xlabel("Conversion (0 = не конвертировался, 1 = конвертировался)")
ax.set_ylabel("Количество наблюдений")
ax.set_title("Распределение целевой переменной Conversion")
for i, v in enumerate(target_counts.values):
    ax.text(i, v + 0.01 * max(target_counts.values), str(v), ha="center", va="bottom")
plt.tight_layout()

target_plot_path = os.path.join(PLOTS_DIR, "target_distribution_conversion.png")
plt.savefig(target_plot_path, dpi=150)
plt.show()
plt.close()

print("[INFO] График распределения таргета сохранён в:", target_plot_path)

conversion_summary_rows = []

for col in categorical_cols:
    # Считаем среднюю конверсию по категориям
    group_df = (
        df.groupby(col)[TARGET_COL]
        .agg(["mean", "count"])
        .rename(columns={"mean": "conversion_rate", "count": "n"})
        .sort_values("conversion_rate", ascending=False)
    )

    print(f"\n[SUMMARY] Конверсия по признаку {col}:")
    display(group_df)

    # Сохраним в список для дальнейшего объединения
    tmp = group_df.reset_index()
    tmp.insert(0, "feature", col)
    conversion_summary_rows.append(tmp)

    # Визуализация
    fig, ax = plt.subplots(figsize=(8, 4))
    sns.barplot(
        data=group_df.reset_index(),
        x=col,
        y="conversion_rate",
        ax=ax,
    )
    ax.set_title(f"Средняя конверсия по категориям признака {col}")
    ax.set_xlabel(col)
    ax.set_ylabel("Conversion rate")

    for i, (category, row) in enumerate(group_df.reset_index()[[col, "conversion_rate"]].values):
        ax.text(i, row + 0.01 * group_df["conversion_rate"].max(), f"{row:.2f}", ha="center", va="bottom")

    plt.xticks(rotation=30, ha="right")
    plt.tight_layout()

    conv_plot_path = os.path.join(PLOTS_DIR, f"conversion_by_{col}.png")
    plt.savefig(conv_plot_path, dpi=150)
    plt.show()
    plt.close()

    print(f"[INFO] Сохранён график конверсии по признаку {col}: {conv_plot_path}")

# Объединяем все summary в один DataFrame и сохраняем
if conversion_summary_rows:
    conv_summary_df = pd.concat(conversion_summary_rows, ignore_index=True)
    conv_summary_path = os.path.join(DATA_DIR, "conversion_summary_by_categorical.csv")
    conv_summary_df.to_csv(conv_summary_path, index=False, encoding="utf-8-sig")
    print(
        "\n[INFO] Таблица с конверсией по категориальным признакам сохранена в:",
        conv_summary_path,
    )
    display(conv_summary_df.head())
else:
    print("[WARN] Нет категориальных признаков для расчёта конверсии.")

corr_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print("\n[INFO] Числовые признаки для корреляции:", corr_cols)

corr_matrix = df[corr_cols].corr()

fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(
    corr_matrix,
    annot=False,
    cmap="coolwarm",
    center=0.0,
    square=True,
    cbar_kws={"shrink": 0.8},
    ax=ax,
)
ax.set_title("Матрица корреляций числовых признаков")
plt.tight_layout()

corr_plot_path = os.path.join(PLOTS_DIR, "correlation_matrix_numeric.png")
plt.savefig(corr_plot_path, dpi=150)
plt.show()
plt.close()

print("[INFO] Матрица корреляций сохранена в:", corr_plot_path)

# --- Пара интересных пар признаков для скеттерплотов / jointplot ---
candidate_pairs = [
    ("AdSpend", "ClickThroughRate"),
    ("WebsiteVisits", "ConversionRate"),
    ("PagesPerVisit", "TimeOnSite"),
    ("PreviousPurchases", "LoyaltyPoints"),
]

for x_col, y_col in candidate_pairs:
    if x_col in df.columns and y_col in df.columns:
        fig, ax = plt.subplots(figsize=(6, 5))
        sns.scatterplot(data=df, x=x_col, y=y_col, hue=TARGET_COL, alpha=0.6, ax=ax)
        ax.set_title(f"Связь между {x_col} и {y_col} (цвет = Conversion)")
        plt.tight_layout()

        pair_plot_path = os.path.join(PLOTS_DIR, f"pair_{x_col}_vs_{y_col}.png")
        plt.savefig(pair_plot_path, dpi=150)
        plt.show()
        plt.close()

        print(
            f"[INFO] Сохранён график связи {x_col} vs {y_col}:",
            pair_plot_path,
        )
    else:
        print(
            f"[WARN] Пара ({x_col}, {y_col}) пропущена: один из признаков отсутствует в df.columns."
        )
