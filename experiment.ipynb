import os
import random
import json
import numpy as np

# Если вы будете использовать torch / tf позже — сразу заложим задел
try:
    import torch
except ImportError:
    torch = None

try:
    import tensorflow as tf
except ImportError:
    tf = None

# --- Подключение Google Drive (работает только в Colab) ---
try:
    from google.colab import drive  # type: ignore
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    drive.mount("/content/drive")
    BASE_DIR = "/content/drive/MyDrive/digital_marketing_conversion"
else:
    # Локальный fallback — можно поменять под себя
    BASE_DIR = os.path.abspath("./digital_marketing_conversion")

DATA_DIR = os.path.join(BASE_DIR, "data")
MODELS_DIR = os.path.join(BASE_DIR, "models")
PLOTS_DIR = os.path.join(BASE_DIR, "plots")
REPORTS_DIR = os.path.join(BASE_DIR, "reports")
ARTIFACTS_DIR = os.path.join(BASE_DIR, "artifacts")

for d in [BASE_DIR, DATA_DIR, MODELS_DIR, PLOTS_DIR, REPORTS_DIR, ARTIFACTS_DIR]:
    os.makedirs(d, exist_ok=True)

print("BASE_DIR:", BASE_DIR)
print("DATA_DIR:", DATA_DIR)
print("MODELS_DIR:", MODELS_DIR)
print("PLOTS_DIR:", PLOTS_DIR)
print("REPORTS_DIR:", REPORTS_DIR)
print("ARTIFACTS_DIR:", ARTIFACTS_DIR)

# --- Функция для установки случайных зерен ---
def set_global_seed(seed: int = 42) -> None:
    random.seed(seed)
    np.random.seed(seed)

    if torch is not None:
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        # Дополнительно чуть жёстче фиксируем детерминизм
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    if tf is not None:
        try:
            tf.random.set_seed(seed)
        except Exception:
            pass

    print(f"[INFO] Глобальное зерно случайности установлено: {seed}")

set_global_seed(42)

# --- Проверка наличия GPU ---
if torch is not None and torch.cuda.is_available():
    print("[INFO] PyTorch видит GPU:", torch.cuda.get_device_name(0))
elif tf is not None and len(tf.config.list_physical_devices("GPU")) > 0:
    print("[INFO] TensorFlow видит GPU:", tf.config.list_physical_devices("GPU"))
else:
    print("[WARN] GPU не обнаружен (или библиотеки DL ещё не установлены).")
    print("       Но для датасета на 8000 строк это не критично.")

# Базовые библиотеки анализа данных и визуализации
!pip install -q pandas numpy matplotlib seaborn

# ML-стек (дальше пригодится, но уже сейчас можно поставить)
!pip install -q scikit-learn imbalanced-learn xgboost lightgbm shap umap-learn

# Работа с Kaggle API и конфигами
!pip install -q kaggle pyyaml

import os
import zipfile
from pathlib import Path

if IN_COLAB:
    from google.colab import files  # type: ignore

    print(
        "Пожалуйста, загрузите kaggle.json (из профиля Kaggle → Account → Create New Token)."
    )
    uploaded = files.upload()  # Откроется диалог выбора файла

    if "kaggle.json" not in uploaded:
        raise FileNotFoundError(
            "Файл kaggle.json не найден среди загруженных. "
            "Убедитесь, что выбрали правильный файл."
        )

    # Настройка Kaggle API
    kaggle_dir = os.path.join(os.path.expanduser("~"), ".kaggle")
    os.makedirs(kaggle_dir, exist_ok=True)

    with open("kaggle.json", "r") as f_in:
        token = json.load(f_in)

    kaggle_json_path = os.path.join(kaggle_dir, "kaggle.json")
    with open(kaggle_json_path, "w") as f_out:
        json.dump(token, f_out)

    os.chmod(kaggle_json_path, 0o600)
    print("[INFO] kaggle.json настроен.")

    # Скачивание датасета
    print("[INFO] Скачиваю датасет 'Predict Conversion in Digital Marketing'...")
    !kaggle datasets download -d rabieelkharoua/predict-conversion-in-digital-marketing-dataset -p {DATA_DIR} --unzip

    # Проверим, какие CSV появились
    csv_files = list(Path(DATA_DIR).glob("*.csv"))
    print("[INFO] Найдены CSV-файлы в DATA_DIR:")
    for p in csv_files:
        print("   -", p.name)

else:
    print(
        "[WARN] Не в Colab: этот блок рассчитан на Google Colab и может не работать локально."
    )

import yaml

dataset_registry_path = os.path.join(DATA_DIR, "dataset_registry.yaml")

# Попробуем автоматически найти основной CSV
from glob import glob

csv_files = glob(os.path.join(DATA_DIR, "*.csv"))

main_dataset_path = None
for candidate in csv_files:
    name = os.path.basename(candidate).lower()
    if "digital_marketing_campaign" in name:
        main_dataset_path = candidate
        break

if main_dataset_path is None and csv_files:
    main_dataset_path = csv_files[0]  # fallback — первый найденный CSV

dataset_registry = {
    "main_conversion_dataset": {
        "source": "kaggle",
        "kaggle_dataset": "rabieelkharoua/predict-conversion-in-digital-marketing-dataset",
        "path": main_dataset_path,
        "description": "Predict Conversion in Digital Marketing Dataset (основной датасет для экспериментов)",
    },
    # Сюда позже можно дописать ещё один датасет (stuffmart, др.)
}

with open(dataset_registry_path, "w", encoding="utf-8") as f:
    yaml.safe_dump(dataset_registry, f, allow_unicode=True)

print("[INFO] dataset_registry.yaml сохранён по пути:", dataset_registry_path)
print("       Основной датасет:", main_dataset_path)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use("default")
sns.set_theme(style="whitegrid")

# Грузим registry, чтобы взять путь к датасету
with open(dataset_registry_path, "r", encoding="utf-8") as f:
    dataset_registry = yaml.safe_load(f)

main_dataset_info = dataset_registry["main_conversion_dataset"]
csv_path = main_dataset_info["path"]

if csv_path is None or not os.path.exists(csv_path):
    raise FileNotFoundError(
        f"CSV-файл основного датасета не найден. Ожидался путь: {csv_path}"
    )

print("[INFO] Загружаю основной датасет из:", csv_path)
df = pd.read_csv(csv_path)

print("\n[SHAPE] Размерность датасета:", df.shape)
print("\n[D_TYPES] Типы данных колонок:")
print(df.dtypes)

print("\n[HEAD] Первые 5 строк:")
display(df.head())

print("\n[NA] Количество пропусков в каждом столбце:")
print(df.isna().sum())

print("\n[DESCRIBE] Базовая статистика по числовым признакам:")
display(df.describe().T)

# --- Data Dictionary / Feature Catalog ---
feature_catalog = pd.DataFrame(
    {
        "feature_name": df.columns,
        "dtype": [str(dt) for dt in df.dtypes],
        "n_unique": [df[col].nunique() for col in df.columns],
        "n_missing": [df[col].isna().sum() for col in df.columns],
        # поле для ручного заполнения в отчёте при необходимости
        "description_ru": ["" for _ in df.columns],
        "example_value": [df[col].iloc[0] for col in df.columns],
    }
)

feature_catalog_path = os.path.join(DATA_DIR, "feature_catalog.csv")
feature_catalog.to_csv(feature_catalog_path, index=False, encoding="utf-8-sig")
print("\n[INFO] Каталог признаков сохранён в:", feature_catalog_path)
display(feature_catalog)

TARGET_COL = "Conversion"

if TARGET_COL not in df.columns:
    raise KeyError(
        f"Ожидалась колонка таргета '{TARGET_COL}', но её нет в df.columns: {df.columns.tolist()}"
    )

target_counts = df[TARGET_COL].value_counts().sort_index()
target_probs = df[TARGET_COL].value_counts(normalize=True).sort_index()

print("\n[INFO] Распределение целевой переменной (сырые значения):")
print(target_counts)
print("\n[INFO] Распределение целевой переменной (доли):")
print(target_probs)

fig, ax = plt.subplots(figsize=(5, 4))
sns.barplot(
    x=target_counts.index.astype(str),
    y=target_counts.values,
    ax=ax,
)
ax.set_xlabel("Conversion (0 = не конвертировался, 1 = конвертировался)")
ax.set_ylabel("Количество наблюдений")
ax.set_title("Распределение целевой переменной Conversion")
for i, v in enumerate(target_counts.values):
    ax.text(i, v + 0.01 * max(target_counts.values), str(v), ha="center", va="bottom")
plt.tight_layout()

target_plot_path = os.path.join(PLOTS_DIR, "target_distribution_conversion.png")
plt.savefig(target_plot_path, dpi=150)
plt.show()
plt.close()

print("[INFO] График распределения таргета сохранён в:", target_plot_path)

conversion_summary_rows = []

for col in categorical_cols:
    # Считаем среднюю конверсию по категориям
    group_df = (
        df.groupby(col)[TARGET_COL]
        .agg(["mean", "count"])
        .rename(columns={"mean": "conversion_rate", "count": "n"})
        .sort_values("conversion_rate", ascending=False)
    )

    print(f"\n[SUMMARY] Конверсия по признаку {col}:")
    display(group_df)

    # Сохраним в список для дальнейшего объединения
    tmp = group_df.reset_index()
    tmp.insert(0, "feature", col)
    conversion_summary_rows.append(tmp)

    # Визуализация
    fig, ax = plt.subplots(figsize=(8, 4))
    sns.barplot(
        data=group_df.reset_index(),
        x=col,
        y="conversion_rate",
        ax=ax,
    )
    ax.set_title(f"Средняя конверсия по категориям признака {col}")
    ax.set_xlabel(col)
    ax.set_ylabel("Conversion rate")

    for i, (category, row) in enumerate(group_df.reset_index()[[col, "conversion_rate"]].values):
        ax.text(i, row + 0.01 * group_df["conversion_rate"].max(), f"{row:.2f}", ha="center", va="bottom")

    plt.xticks(rotation=30, ha="right")
    plt.tight_layout()

    conv_plot_path = os.path.join(PLOTS_DIR, f"conversion_by_{col}.png")
    plt.savefig(conv_plot_path, dpi=150)
    plt.show()
    plt.close()

    print(f"[INFO] Сохранён график конверсии по признаку {col}: {conv_plot_path}")

# Объединяем все summary в один DataFrame и сохраняем
if conversion_summary_rows:
    conv_summary_df = pd.concat(conversion_summary_rows, ignore_index=True)
    conv_summary_path = os.path.join(DATA_DIR, "conversion_summary_by_categorical.csv")
    conv_summary_df.to_csv(conv_summary_path, index=False, encoding="utf-8-sig")
    print(
        "\n[INFO] Таблица с конверсией по категориальным признакам сохранена в:",
        conv_summary_path,
    )
    display(conv_summary_df.head())
else:
    print("[WARN] Нет категориальных признаков для расчёта конверсии.")

corr_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print("\n[INFO] Числовые признаки для корреляции:", corr_cols)

corr_matrix = df[corr_cols].corr()

fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(
    corr_matrix,
    annot=False,
    cmap="coolwarm",
    center=0.0,
    square=True,
    cbar_kws={"shrink": 0.8},
    ax=ax,
)
ax.set_title("Матрица корреляций числовых признаков")
plt.tight_layout()

corr_plot_path = os.path.join(PLOTS_DIR, "correlation_matrix_numeric.png")
plt.savefig(corr_plot_path, dpi=150)
plt.show()
plt.close()

print("[INFO] Матрица корреляций сохранена в:", corr_plot_path)

# --- Пара интересных пар признаков для скеттерплотов / jointplot ---
candidate_pairs = [
    ("AdSpend", "ClickThroughRate"),
    ("WebsiteVisits", "ConversionRate"),
    ("PagesPerVisit", "TimeOnSite"),
    ("PreviousPurchases", "LoyaltyPoints"),
]

for x_col, y_col in candidate_pairs:
    if x_col in df.columns and y_col in df.columns:
        fig, ax = plt.subplots(figsize=(6, 5))
        sns.scatterplot(data=df, x=x_col, y=y_col, hue=TARGET_COL, alpha=0.6, ax=ax)
        ax.set_title(f"Связь между {x_col} и {y_col} (цвет = Conversion)")
        plt.tight_layout()

        pair_plot_path = os.path.join(PLOTS_DIR, f"pair_{x_col}_vs_{y_col}.png")
        plt.savefig(pair_plot_path, dpi=150)
        plt.show()
        plt.close()

        print(
            f"[INFO] Сохранён график связи {x_col} vs {y_col}:",
            pair_plot_path,
        )
    else:
        print(
            f"[WARN] Пара ({x_col}, {y_col}) пропущена: один из признаков отсутствует в df.columns."
        )

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# На всякий случай ещё раз зададим некоторые базовые объекты
plt.style.use("default")
sns.set_theme(style="whitegrid")

# Если вы меняли TARGET_COL выше — обязательно синхронизируйте здесь
TARGET_COL = "Conversion"

def add_business_features(df_in: pd.DataFrame) -> tuple[pd.DataFrame, list[str]]:
    """
    Добавляет бизнес-ориентированные производные признаки.
    Возвращает новый DataFrame и список созданных колонок.
    """
    df_out = df_in.copy()
    derived_cols: list[str] = []
    EPS = 1e-6

    # 1) Метрики e-mail вовлечённости
    if {"EmailClicks", "EmailOpens"}.issubset(df_out.columns):
        df_out["EmailEngagementRate"] = df_out["EmailClicks"] / (df_out["EmailOpens"] + 1.0)
        derived_cols.append("EmailEngagementRate")

    # 2) Метрики социальных сетей
    if {"SocialShares", "WebsiteVisits"}.issubset(df_out.columns):
        df_out["SocialShareRate"] = df_out["SocialShares"] / (df_out["WebsiteVisits"] + 1.0)
        derived_cols.append("SocialShareRate")

    # 3) Стоимость контакта/визита
    if {"AdSpend", "WebsiteVisits"}.issubset(df_out.columns):
        df_out["CostPerVisit"] = df_out["AdSpend"] / (df_out["WebsiteVisits"] + 1.0)
        derived_cols.append("CostPerVisit")

    if {"AdSpend", "EmailClicks"}.issubset(df_out.columns):
        df_out["CostPerEmailClick"] = df_out["AdSpend"] / (df_out["EmailClicks"] + 1.0)
        derived_cols.append("CostPerEmailClick")

    # 4) Показатель лояльности
    if {"LoyaltyPoints", "PreviousPurchases"}.issubset(df_out.columns):
        df_out["LoyaltyPerPurchase"] = df_out["LoyaltyPoints"] / (df_out["PreviousPurchases"] + 1.0)
        derived_cols.append("LoyaltyPerPurchase")

    # 5) Интегральный индекс вовлечённости
    engagement_components = [
        c
        for c in [
            "WebsiteVisits",
            "PagesPerVisit",
            "TimeOnSite",
            "SocialShares",
            "EmailOpens",
            "EmailClicks",
            "ClickThroughRate",
            "ConversionRate",
        ]
        if c in df_out.columns
    ]
    if engagement_components:
        # Простейшая нормализация через ранги (0..1) и среднее
        df_out["EngagementScore"] = (
            df_out[engagement_components]
            .rank(pct=True)
            .mean(axis=1)
        )
        derived_cols.append("EngagementScore")

    print("\n[INFO] Добавлены производные признаки:")
    for col in derived_cols:
        print("   -", col)

    return df_out, derived_cols


# Применяем функцию к исходному df
df_fe, DERIVED_FEATURES = add_business_features(df)

print("\n[SHAPE] После добавления признаков размерность:", df_fe.shape)
print("[INFO] Список производных признаков:", DERIVED_FEATURES)

import os
import json
import numpy as np
import pandas as pd

# Колонка ID (если есть)
ID_COL = "CustomerID" if "CustomerID" in df_fe.columns else None
if ID_COL:
    print(f"\n[INFO] Обнаружена ID-колонка: {ID_COL}")
else:
    print("\n[WARN] Явная ID-колонка не найдена (CustomerID отсутствует).")

# --- Удаляем константные признаки (все значения одинаковые) ---
nunique = df_fe.nunique(dropna=False)
constant_cols = [
    c for c in df_fe.columns
    if (nunique[c] == 1) and (c not in [ID_COL, TARGET_COL])
]

if constant_cols:
    print("\n[INFO] Найдены константные признаки, будем удалять их из признакового пространства:")
    for c in constant_cols:
        print("   -", c)
    df_fe = df_fe.drop(columns=constant_cols)
else:
    print("\n[INFO] Константные признаки не обнаружены.")

# Числовые и категориальные признаки
numeric_cols_all = df_fe.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols_all = [c for c in numeric_cols_all if c not in [TARGET_COL, ID_COL]]

categorical_cols_all = df_fe.select_dtypes(include=["object", "category"]).columns.tolist()

print("\n[INFO] Числовые признаки (кандидаты для масштабирования):")
print(numeric_cols_all)
print("\n[INFO] Категориальные признаки (кандидаты для one-hot):")
print(categorical_cols_all)

# Мягкая обработка выбросов: усечение по 1-му и 99-му перцентилям
winsor_config: dict[str, dict[str, float]] = {}
LOW_Q = 0.01
HIGH_Q = 0.99

for col in numeric_cols_all:
    q_low = df_fe[col].quantile(LOW_Q)
    q_high = df_fe[col].quantile(HIGH_Q)

    outlier_mask = (df_fe[col] < q_low) | (df_fe[col] > q_high)
    outlier_pct = float(outlier_mask.mean() * 100)

    print(
        f"[OUTLIERS] {col}: {outlier_pct:.2f}% наблюдений за пределами "
        f"[{LOW_Q:.2f}; {HIGH_Q:.2f}] перцентилей ({q_low:.3f}, {q_high:.3f})"
    )

    df_fe[col] = df_fe[col].clip(lower=q_low, upper=q_high)

    winsor_config[col] = {
        "q_low": float(q_low),
        "q_high": float(q_high),
        "outlier_pct": outlier_pct,
    }

winsor_config_path = os.path.join(DATA_DIR, "winsorization_config.json")
with open(winsor_config_path, "w", encoding="utf-8") as f:
    json.dump(winsor_config, f, indent=2, ensure_ascii=False)

print("\n[INFO] Конфигурация усечения выбросов сохранена в:", winsor_config_path)

# Финальные списки признаков
NUMERIC_FEATURES = numeric_cols_all.copy()
CATEGORICAL_FEATURES = categorical_cols_all.copy()

print("\n[FEATURES] Числовые признаки для препроцессинга:")
print(NUMERIC_FEATURES)
print("\n[FEATURES] Категориальные признаки для препроцессинга:")
print(CATEGORICAL_FEATURES)

from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import sklearn

# подбираем правильный аргумент для OneHotEncoder в зависимости от версии sklearn
skl_version = sklearn.__version__
major, minor, *_ = skl_version.split(".")
major = int(major)
minor = int(minor)

onehot_kwargs = dict(
    handle_unknown="ignore",
    drop="first",
)

# новые версии (≈>=1.2) используют sparse_output, старые — sparse
if major > 1 or (major == 1 and minor >= 2):
    onehot_kwargs["sparse_output"] = False
else:
    onehot_kwargs["sparse"] = False

print(f"[INFO] Версия scikit-learn: {skl_version}")
print("[INFO] Параметры OneHotEncoder:", onehot_kwargs)

numeric_transformer = Pipeline(
    steps=[
        ("scaler", RobustScaler()),
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ("onehot", OneHotEncoder(**onehot_kwargs)),
    ]
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, NUMERIC_FEATURES),
        ("cat", categorical_transformer, CATEGORICAL_FEATURES),
    ]
)

# Описание пространства признаков для Flask и отчёта
feature_space = {
    "id_col": ID_COL,
    "target_col": TARGET_COL,
    "numeric_features": NUMERIC_FEATURES,
    "categorical_features": CATEGORICAL_FEATURES,
    "derived_features": DERIVED_FEATURES,
    "constant_dropped": constant_cols,
    "winsorization": winsor_config,
    "scaler": "RobustScaler",
    "encoder": {
        "class": "OneHotEncoder",
        "params": onehot_kwargs,
    },
}

feature_space_path = os.path.join(DATA_DIR, "feature_space.json")
with open(feature_space_path, "w", encoding="utf-8") as f:
    json.dump(feature_space, f, indent=2, ensure_ascii=False)

print("\n[INFO] Описание пространства признаков сохранено в:", feature_space_path)

# Быстрая проверка работы препроцессора
sample_size = min(1000, len(df_fe))
X_sample = df_fe[NUMERIC_FEATURES + CATEGORICAL_FEATURES].iloc[:sample_size]
print(f"\n[DEBUG] Пробный препроцессинг на {sample_size} наблюдениях...")
X_sample_transformed = preprocess.fit_transform(X_sample)
print("   Исходная форма X_sample:", X_sample.shape)
print("   Форма после preprocess:", X_sample_transformed.shape)

from sklearn.model_selection import train_test_split

feature_cols_for_model = NUMERIC_FEATURES + CATEGORICAL_FEATURES

X = df_fe[feature_cols_for_model].copy()
y = df_fe[TARGET_COL].astype(int)

print("\n[SHAPE] Размерности X и y перед разбиением:")
print("   X:", X.shape)
print("   y:", y.shape)

X_trainval, X_test, y_trainval, y_test = train_test_split(
    X,
    y,
    test_size=0.20,
    random_state=42,
    stratify=y,
)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_trainval,
    y_trainval,
    test_size=0.25,  # 0.25 от 0.8 = 0.2 общей выборки
    random_state=42,
    stratify=y_trainval,
)

print("\n[SHAPE] Итоговые размерности выборок:")
print("   X_train:", X_train.shape, "| y_train:", y_train.shape)
print("   X_valid:", X_valid.shape, "| y_valid:", y_valid.shape)
print("   X_test :", X_test.shape,  "| y_test :", y_test.shape)

def print_class_balance(name: str, y_part: pd.Series) -> None:
    vc = y_part.value_counts(normalize=True).sort_index()
    print(f"   [{name}] распределение классов:")
    for cls, p in vc.items():
        print(f"      класс {cls}: {p*100:.2f}%")

print()
print_class_balance("train", y_train)
print_class_balance("valid", y_valid)
print_class_balance("test", y_test)

splits_indices = {
    "train": X_train.index.tolist(),
    "valid": X_valid.index.tolist(),
    "test": X_test.index.tolist(),
}

split_indices_path = os.path.join(DATA_DIR, "split_indices.json")
with open(split_indices_path, "w", encoding="utf-8") as f:
    json.dump(splits_indices, f, indent=2, ensure_ascii=False)

print("\n[INFO] Индексы разбиений train/valid/test сохранены в:", split_indices_path)

if ID_COL is not None:
    for name, idx in splits_indices.items():
        subset = df_fe.loc[idx, [ID_COL, TARGET_COL]]
        out_path = os.path.join(DATA_DIR, f"{name}_id_target.csv")
        subset.to_csv(out_path, index=False, encoding="utf-8-sig")
        print(f"[INFO] Для {name} сохранён файл ID+таргет:", out_path)
else:
    print("\n[WARN] ID-колонка не задана, отдельные файлы ID+таргет не сохранялись.")

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    average_precision_score,
    roc_curve,
    precision_recall_curve,
    confusion_matrix,
)

plt.style.use("default")

def _compute_basic_metrics(y_true: np.ndarray, y_proba: np.ndarray, threshold: float = 0.5) -> dict:
    """
    Считает набор стандартных метрик бинарной классификации.
    y_proba — вероятности класса 1.
    """
    y_pred = (y_proba >= threshold).astype(int)

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    roc = roc_auc_score(y_true, y_proba)
    pr_auc = average_precision_score(y_true, y_proba)

    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    metrics = {
        "threshold": threshold,
        "accuracy": acc,
        "f1": f1,
        "precision": prec,
        "recall": rec,
        "roc_auc": roc,
        "pr_auc": pr_auc,
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn),
        "tp": int(tp),
        "support_neg": int(tn + fp),
        "support_pos": int(fn + tp),
    }
    return metrics


def plot_roc_pr_curves(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    model_name: str,
    split_name: str,
    plots_dir: str,
) -> dict:
    """
    Строит ROC- и PR-кривые, сохраняет их в файлы и возвращает пути.
    """
    model_key = model_name.lower().replace(" ", "_")
    split_key = split_name.lower()

    # ROC
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    fig, ax = plt.subplots(figsize=(6, 5))
    ax.plot(fpr, tpr, label=f"{model_name}")
    ax.plot([0, 1], [0, 1], linestyle="--", label="Случайный классификатор")
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate (Recall)")
    ax.set_title(f"ROC-кривая ({model_name}, {split_name})")
    ax.legend()
    plt.tight_layout()
    roc_path = os.path.join(plots_dir, f"roc_{model_key}_{split_key}.png")
    plt.savefig(roc_path, dpi=150)
    plt.show()
    plt.close()

    # PR
    precision, recall, _ = precision_recall_curve(y_true, y_proba)
    fig, ax = plt.subplots(figsize=(6, 5))
    ax.plot(recall, precision, label=f"{model_name}")
    base_rate = (y_true == 1).mean()
    ax.hlines(base_rate, 0, 1, linestyles="--", label=f"Базовая линия (p={base_rate:.3f})")
    ax.set_xlabel("Recall")
    ax.set_ylabel("Precision")
    ax.set_title(f"Precision-Recall-кривая ({model_name}, {split_name})")
    ax.legend()
    plt.tight_layout()
    pr_path = os.path.join(plots_dir, f"pr_{model_key}_{split_key}.png")
    plt.savefig(pr_path, dpi=150)
    plt.show()
    plt.close()

    return {"roc_path": roc_path, "pr_path": pr_path}


def evaluate_binary_model(
    model: Pipeline,
    model_name: str,
    X_train,
    y_train,
    X_valid,
    y_valid,
    X_test,
    y_test,
    plots_dir: str,
    threshold: float = 0.5,
) -> dict:
    """
    Обучает модель и считает метрики на train / valid / test.
    Возвращает словарь с метриками и путями к картинкам ROC/PR для теста.
    """
    print(f"\n[INFO] Обучение модели: {model_name}")
    model.fit(X_train, y_train)

    # Предсказания вероятностей (класс 1)
    if hasattr(model, "predict_proba"):
        proba_train = model.predict_proba(X_train)[:, 1]
        proba_valid = model.predict_proba(X_valid)[:, 1]
        proba_test = model.predict_proba(X_test)[:, 1]
    else:
        # На всякий случай fallback через decision_function
        decision_train = model.decision_function(X_train)
        decision_valid = model.decision_function(X_valid)
        decision_test = model.decision_function(X_test)
        # нормируем до 0..1 (логистика)
        proba_train = 1 / (1 + np.exp(-decision_train))
        proba_valid = 1 / (1 + np.exp(-decision_valid))
        proba_test = 1 / (1 + np.exp(-decision_test))

    metrics_train = _compute_basic_metrics(y_train, proba_train, threshold)
    metrics_valid = _compute_basic_metrics(y_valid, proba_valid, threshold)
    metrics_test = _compute_basic_metrics(y_test, proba_test, threshold)

    print(f"[METRICS] {model_name} — validation:")
    for k, v in metrics_valid.items():
        if k in ["tn", "fp", "fn", "tp", "support_neg", "support_pos"]:
            continue
        print(f"   {k:10s}: {v:.4f}" if isinstance(v, float) else f"   {k:10s}: {v}")

    # ROC/PR для теста
    plot_paths = plot_roc_pr_curves(
        y_true=y_test,
        y_proba=proba_test,
        model_name=model_name,
        split_name="test",
        plots_dir=plots_dir,
    )

    return {
        "train": metrics_train,
        "valid": metrics_valid,
        "test": metrics_test,
        "plots": plot_paths,
    }


# Словарь, куда будем складывать результаты по моделям
baseline_results: dict[str, dict] = {}

dummy_clf = DummyClassifier(strategy="most_frequent")

dummy_pipeline = Pipeline(
    steps=[
        ("preprocess", preprocess),
        ("clf", dummy_clf),
    ]
)

baseline_results["Dummy_most_frequent"] = evaluate_binary_model(
    model=dummy_pipeline,
    model_name="Dummy (most frequent)",
    X_train=X_train,
    y_train=y_train,
    X_valid=X_valid,
    y_valid=y_valid,
    X_test=X_test,
    y_test=y_test,
    plots_dir=PLOTS_DIR,
    threshold=0.5,
)

# 1) Логистическая регрессия без учёта дисбаланса
logreg_plain = LogisticRegression(
    penalty="l2",
    C=1.0,
    solver="liblinear",
    max_iter=1000,
    n_jobs=-1,
)

logreg_plain_pipeline = Pipeline(
    steps=[
        ("preprocess", preprocess),
        ("clf", logreg_plain),
    ]
)

baseline_results["LogReg_plain"] = evaluate_binary_model(
    model=logreg_plain_pipeline,
    model_name="LogReg (plain)",
    X_train=X_train,
    y_train=y_train,
    X_valid=X_valid,
    y_valid=y_valid,
    X_test=X_test,
    y_test=y_test,
    plots_dir=PLOTS_DIR,
    threshold=0.5,
)

# 2) Логистическая регрессия с учётом дисбаланса классов
logreg_bal = LogisticRegression(
    penalty="l2",
    C=1.0,
    solver="liblinear",
    max_iter=1000,
    n_jobs=-1,
    class_weight="balanced",
)

logreg_bal_pipeline = Pipeline(
    steps=[
        ("preprocess", preprocess),
        ("clf", logreg_bal),
    ]
)

baseline_results["LogReg_balanced"] = evaluate_binary_model(
    model=logreg_bal_pipeline,
    model_name="LogReg (balanced)",
    X_train=X_train,
    y_train=y_train,
    X_valid=X_valid,
    y_valid=y_valid,
    X_test=X_test,
    y_test=y_test,
    plots_dir=PLOTS_DIR,
    threshold=0.5,
)

# Преобразуем baseline_results в таблицу
rows = []
for model_name, res in baseline_results.items():
    for split_name in ["train", "valid", "test"]:
        metrics = res[split_name]
        row = {
            "model": model_name,
            "split": split_name,
        }
        row.update(metrics)
        rows.append(row)

metrics_df = pd.DataFrame(rows)

print("\n[INFO] Сводная таблица метрик базовых моделей:")
display(metrics_df)

# Сохраняем в CSV и JSON
os.makedirs(REPORTS_DIR, exist_ok=True)

metrics_csv_path = os.path.join(REPORTS_DIR, "metrics_baselines.csv")
metrics_df.to_csv(metrics_csv_path, index=False, encoding="utf-8-sig")

metrics_json_path = os.path.join(REPORTS_DIR, "metrics_baselines.json")
with open(metrics_json_path, "w", encoding="utf-8") as f:
    json.dump(baseline_results, f, indent=2, ensure_ascii=False)

print("\n[INFO] Метрики базовых моделей сохранены в:")
print("   CSV :", metrics_csv_path)
print("   JSON:", metrics_json_path)
